{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10 : Assocation rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shrimp,almonds,avocado,vegetables mix,green gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers,meatballs,eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chutney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>turkey,avocado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mineral water,milk,energy bar,whole wheat rice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>butter,light mayo,fresh bread</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>burgers,frozen vegetables,eggs,french fries,ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>chicken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>escalope,green tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>eggs,frozen smoothie,yogurt cake,low fat yogurt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7501 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     shrimp,almonds,avocado,vegetables mix,green gr...\n",
       "1                                burgers,meatballs,eggs\n",
       "2                                               chutney\n",
       "3                                        turkey,avocado\n",
       "4     mineral water,milk,energy bar,whole wheat rice...\n",
       "...                                                 ...\n",
       "7496                      butter,light mayo,fresh bread\n",
       "7497  burgers,frozen vegetables,eggs,french fries,ma...\n",
       "7498                                            chicken\n",
       "7499                                 escalope,green tea\n",
       "7500    eggs,frozen smoothie,yogurt cake,low fat yogurt\n",
       "\n",
       "[7501 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Online retail.xlsx',header=None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded DataFrame:\n",
      "   Apple   Corn   Dill   Eggs  Ice cream  Kidney Beans   Milk  Nutmeg  Onion  \\\n",
      "0  False  False  False   True      False          True   True    True   True   \n",
      "1  False  False   True   True      False          True  False    True   True   \n",
      "2   True  False  False   True      False          True   True   False  False   \n",
      "3  False   True  False  False      False          True   True   False  False   \n",
      "4  False   True  False   True       True          True  False   False   True   \n",
      "\n",
      "   Unicorn  Yogurt  \n",
      "0    False    True  \n",
      "1    False    True  \n",
      "2    False   False  \n",
      "3     True    True  \n",
      "4    False   False  \n"
     ]
    }
   ],
   "source": [
    "# Initialize TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "\n",
    "# Fit and transform the dataset\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(\"One-Hot Encoded DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets (min_support=0.6):\n",
      "    support                     itemsets\n",
      "0       0.8                       (Eggs)\n",
      "1       1.0               (Kidney Beans)\n",
      "2       0.6                       (Milk)\n",
      "3       0.6                      (Onion)\n",
      "4       0.6                     (Yogurt)\n",
      "5       0.8         (Eggs, Kidney Beans)\n",
      "6       0.6                (Eggs, Onion)\n",
      "7       0.6         (Milk, Kidney Beans)\n",
      "8       0.6        (Onion, Kidney Beans)\n",
      "9       0.6       (Yogurt, Kidney Beans)\n",
      "10      0.6  (Eggs, Onion, Kidney Beans)\n"
     ]
    }
   ],
   "source": [
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "# Set min_support to 0.6 (i.e., itemset must appear in at least 60% of transactions)\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "\n",
    "print(\"Frequent Itemsets (min_support=0.6):\")\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association Rules (min_confidence=0.7):\n",
      "              antecedents            consequents  antecedent support  \\\n",
      "0                  (Eggs)         (Kidney Beans)                 0.8   \n",
      "1          (Kidney Beans)                 (Eggs)                 1.0   \n",
      "2                  (Eggs)                (Onion)                 0.8   \n",
      "3                 (Onion)                 (Eggs)                 0.6   \n",
      "4                  (Milk)         (Kidney Beans)                 0.6   \n",
      "5                 (Onion)         (Kidney Beans)                 0.6   \n",
      "6                (Yogurt)         (Kidney Beans)                 0.6   \n",
      "7           (Eggs, Onion)         (Kidney Beans)                 0.6   \n",
      "8    (Eggs, Kidney Beans)                (Onion)                 0.8   \n",
      "9   (Onion, Kidney Beans)                 (Eggs)                 0.6   \n",
      "10                 (Eggs)  (Onion, Kidney Beans)                 0.8   \n",
      "11                (Onion)   (Eggs, Kidney Beans)                 0.6   \n",
      "\n",
      "    consequent support  support  confidence  lift  representativity  leverage  \\\n",
      "0                  1.0      0.8        1.00  1.00               1.0      0.00   \n",
      "1                  0.8      0.8        0.80  1.00               1.0      0.00   \n",
      "2                  0.6      0.6        0.75  1.25               1.0      0.12   \n",
      "3                  0.8      0.6        1.00  1.25               1.0      0.12   \n",
      "4                  1.0      0.6        1.00  1.00               1.0      0.00   \n",
      "5                  1.0      0.6        1.00  1.00               1.0      0.00   \n",
      "6                  1.0      0.6        1.00  1.00               1.0      0.00   \n",
      "7                  1.0      0.6        1.00  1.00               1.0      0.00   \n",
      "8                  0.6      0.6        0.75  1.25               1.0      0.12   \n",
      "9                  0.8      0.6        1.00  1.25               1.0      0.12   \n",
      "10                 0.6      0.6        0.75  1.25               1.0      0.12   \n",
      "11                 0.8      0.6        1.00  1.25               1.0      0.12   \n",
      "\n",
      "    conviction  zhangs_metric  jaccard  certainty  kulczynski  \n",
      "0          inf            0.0     0.80      0.000       0.900  \n",
      "1          1.0            0.0     0.80      0.000       0.900  \n",
      "2          1.6            1.0     0.75      0.375       0.875  \n",
      "3          inf            0.5     0.75      1.000       0.875  \n",
      "4          inf            0.0     0.60      0.000       0.800  \n",
      "5          inf            0.0     0.60      0.000       0.800  \n",
      "6          inf            0.0     0.60      0.000       0.800  \n",
      "7          inf            0.0     0.60      0.000       0.800  \n",
      "8          1.6            1.0     0.75      0.375       0.875  \n",
      "9          inf            0.5     0.75      1.000       0.875  \n",
      "10         1.6            1.0     0.75      0.375       0.875  \n",
      "11         inf            0.5     0.75      1.000       0.875  \n"
     ]
    }
   ],
   "source": [
    "# Generate association rules from frequent itemsets\n",
    "# Set metric to \"confidence\" and min_threshold to 0.7\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(\"Association Rules (min_confidence=0.7):\")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Association rule mining using the Apriori algorithm effectively identifies patterns and relationships in transactional data.\n",
    "- Insights from these rules can be applied to improve marketing strategies, cross-selling, and inventory management.\n",
    "- Balancing computational efficiency with interpretability is crucial for practical implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) What is lift and why is it important in Association rules?\n",
    "## Ans = \n",
    "## 1) lift :\n",
    "- Lift is the ratio of the observed confidence of an association to the expected confidence if the items were independent. It's calculated by dividing the confidence of the association by the support of the second item. \n",
    "## 2) interpretion of lift :\n",
    "- A lift value greater than 1 indicates a stronger than expected association, while a lift value less than 1 indicates a negative correlation. A lift value of 1 means the association is no better than chance. \n",
    "## 3) lift importance :\n",
    "- Lift helps determine if an association is meaningful and worth further investigation. For example, a store manager can use lift values to decide how to place products on aisles. \n",
    "## 4) Limitations of lift :\n",
    "- A high lift value doesn't necessarily indicate how frequent or relevant the association is. It's also important to question the reasons behind an association and avoid making misguided decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)What is support and Confidence. How do you calculate them?\n",
    "## Ans =\n",
    "### Support\n",
    "- Support is a measure of the number of times an item set appears in a dataset.\n",
    "- Support is calculated by dividing the number of transactions containing an item set by the total number of transactions.\n",
    "- Support is used to identify itemsets that occur frequently in the dataset.\n",
    "- Support is often used with a threshold to identify itemsets that occur frequently enough to be of interest.\n",
    "- Support is interpreted as the percentage of transactions in which an item set appears.\n",
    "### Confidence\n",
    "- Confidence is a measure of the likelihood that an itemset will appear if another itemset appears.\n",
    "- Confidence is calculated by dividing the number of transactions containing both itemsets by the number of transactions containing the first itemset.\n",
    "- Confidence is used to evaluate the strength of a rule.\n",
    "- Confidence is often used with a threshold to identify rules that are strong enough to be of interest.\n",
    "- Confidence is interpreted as the percentage of transactions in which the second itemset appears given that the first itemset appears.\n",
    "### how to calculate\n",
    "#### Support\n",
    "- Understand the dataset: A dataset is usually a list of transactions. Each transaction contains a set of items purchased together.\n",
    "- Identify the item or itemset: Decide which item(s) you want to evaluate.\n",
    "-  Count how many transactions include the item or itemset, divide by the total number of transactions.\n",
    "#### Confidence\n",
    "- Identify the rule: Association rules are written as \n",
    "- Find support of both sets:\n",
    "- Support for X: Count transactions that contain\n",
    "- Support for 𝑋 ∪ 𝑌 : Count transactions that contain both X and 𝑌\n",
    "- Divide the support of the combined items X ∪ Y by the support of the antecedent 𝑋\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) What are some limitations or challenges of Association rules mining?\n",
    "## Ans =\n",
    "### Association rule mining is a technique used to identify relationships between variables in large datasets. However, it has several limitations:\n",
    "\n",
    "### Parameter Selection: \n",
    "- Choosing appropriate thresholds for support and confidence can be challenging, especially for users without data mining expertise. Inappropriate settings may lead to either too many insignificant rules or missing important ones. \n",
    "\n",
    "### Volume of Rules: \n",
    "- The process can generate an overwhelming number of rules, many of which may be trivial or non-actionable. This abundance can make it challenging to identify truly valuable insights. \n",
    "\n",
    "### Computational Complexity: \n",
    "- Mining association rules, especially in large datasets, is computationally intensive. Algorithms like Apriori require multiple database scans, leading to high time and space complexity. \n",
    "\n",
    "### Data Quality Issues: \n",
    "- The effectiveness of association rule mining is heavily dependent on the quality of the data. Noisy, incomplete, or biased data can result in misleading rules. \n",
    "\n",
    "### Scalability: \n",
    "- As datasets grow in size and dimensionality, traditional association rule mining algorithms may struggle to scale effectively, leading to performance bottlenecks. \n",
    "\n",
    "### Dynamic Data: \n",
    "- Traditional algorithms are designed for static datasets and may not adapt well to data that changes over time, such as data streams, requiring more advanced techniques to handle evolving patterns. \n",
    "\n",
    "\n",
    "### Interpretability: \n",
    "- The sheer number of generated rules can make it difficult for users to interpret and apply the findings effectively, especially when many rules are complex or counterintuitive. \n",
    "\n",
    "### False Positives: \n",
    "- Without proper statistical controls, association rule mining can produce a significant number of false positives—rules that appear significant but are actually due to random chance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
